{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from scipy.stats import skew, boxcox\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy import sparse\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgbm_eval_mae(yhat, dtrain, lift=200):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-lift, np.exp(yhat)-lift), False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data finished in 2.907s\n",
      "Loading test data finished in 4.589s\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "start = time.time() \n",
    "train_data = pd.read_csv('../input/train.csv')\n",
    "train_size=train_data.shape[0]\n",
    "print (\"Loading train data finished in %0.3fs\" % (time.time() - start))        \n",
    "\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "print (\"Loading test data finished in %0.3fs\" % (time.time() - start))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge train and test\n",
    "\n",
    "This will save our time on duplicating logics for train and test and will also ensure the transformations applied on train and test are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data set created.\n"
     ]
    }
   ],
   "source": [
    "full_data=pd.concat([train_data,test_data])\n",
    "del( train_data, test_data)\n",
    "print (\"Full Data set created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_types = full_data.dtypes  \n",
    "cat_cols = list(data_types[data_types=='object'].index)\n",
    "num_cols = list(data_types[data_types=='int64'].index) + list(data_types[data_types=='float64'].index)\n",
    "\n",
    "id_col = 'id'\n",
    "target_col = 'loss'\n",
    "num_cols.remove('id')\n",
    "num_cols.remove('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric features\n",
    "\n",
    "Two preprocessings on numeric features are applied:\n",
    "\n",
    "   1. Apply box-cox transformations for skewed numeric features.\n",
    "\n",
    "   2. Scale numeric features so they will fall in the range between 0 and 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skewed_cols = full_data[num_cols].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "SSL = preprocessing.StandardScaler()\n",
    "skewed_cols = skewed_cols[skewed_cols > 0.25].index.values\n",
    "for skewed_col in skewed_cols:\n",
    "    full_data[skewed_col], lam = boxcox(full_data[skewed_col] + 1)\n",
    "for num_col in num_cols:\n",
    "    full_data[num_col] = SSL.fit_transform(full_data[num_col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model LE Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical features\n",
    "\n",
    "   1. Label Encoding (Factorizing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label enconding finished in 36.702984 seconds\n"
     ]
    }
   ],
   "source": [
    "LBL = preprocessing.LabelEncoder()\n",
    "start=time.time()\n",
    "for cat_col in cat_cols:\n",
    "    full_data[cat_col] = LBL.fit_transform(full_data[cat_col])\n",
    "print ('Label enconding finished in %f seconds' % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lift = 200\n",
    "\n",
    "full_cols = cat_cols+num_cols\n",
    "train_x = full_data[full_cols][:train_size]\n",
    "test_x = full_data[full_cols][train_size:]\n",
    "train_y = np.log(full_data[:train_size].loss.values + lift)\n",
    "ID = full_data.id[:train_size].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual tuning \n",
    "1. [Bayersian Optimization](https://github.com/fmfn/BayesianOptimization) will be introduced to optimize each parameter. \n",
    "2. Based on holdout data (X_val, y_val) for validation for the sake of time. The range for each parameter can be estimated based on the manual tuning results. However, grid search can also be used if time allows.\n",
    "3. A larger learning rate (0.1) is implemented since it requires less iterations to complete. A smaller one (0.01 or even smaller) will be used to train the model for better accuracy when the parameters are optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Tuning\n",
    "\n",
    "* [LightGBM](https://github.com/Microsoft/LightGBM)\n",
    "\n",
    "LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "    * Faster training speed and higher efficiency\n",
    "    * Lower memory usage\n",
    "    * Better accuracy\n",
    "    * Parallel learning supported\n",
    "    * Capable of handling large-scale data\n",
    "12/02/2016 : Release  beta version [python-package](https://github.com/Microsoft/LightGBM/tree/master/python-package)\n",
    "    * The parameters were tuned with pyLightGBM in the Kaggle competition. Feel free to follow the instruction and tune them by yourself.\n",
    "    * The MAE values were different between official python-package and pyLightGBM. However, the parameters were close.\n",
    "\n",
    "\n",
    "* [pyLightGBM](https://github.com/ArdalanM/pyLightGBM)\n",
    "\n",
    "pyLightGBM is a python binding for Microsoft LightGBM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tune num_leaves\n",
    "* default=31, type=int, alias=num_leaf\n",
    "* number of leaves in one tree\n",
    "* control overfit\n",
    "    * Smaller: underfit\n",
    "    * larger: overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_round:  122\n",
      "num_leaves:  127\n",
      "MAE       :  1157.29\n"
     ]
    }
   ],
   "source": [
    "rgr = lgb.LGBMRegressor(learning_rate=0.1,                             \n",
    "                        n_estimators=100000,\n",
    "                        num_leaves=127)\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric=lgbm_eval_mae,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose = False)\n",
    "\n",
    "print 'best_round: ', rgr.best_iteration\n",
    "print 'num_leaves: ', rgr.num_leaves\n",
    "print 'MAE       : ', rgr.evals_result_.get('valid_0').get('mae')[rgr.best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_leaves = 127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tune min_child_samples\n",
    "* default=10, type=int, alias=min_data_per_leaf , min_data\n",
    "* Minimal number of data in one leaf. Can use this to deal with over-fit.\n",
    "* control overfit\n",
    "    * Smaller: overfit\n",
    "    * larger: underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_round:  128\n",
      "min_child_samples:  160\n",
      "MAE       :  1152.27\n"
     ]
    }
   ],
   "source": [
    "rgr = lgb.LGBMRegressor(learning_rate=0.1,                             \n",
    "                        n_estimators=100000,\n",
    "                        num_leaves=num_leaves,\n",
    "                        min_child_samples = 160)\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric=lgbm_eval_mae,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose = False)\n",
    "\n",
    "print 'best_round: ', rgr.best_iteration\n",
    "print 'min_child_samples: ', rgr.min_child_samples\n",
    "print 'MAE       : ', rgr.evals_result_.get('valid_0').get('mae')[rgr.best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_child_samples = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tune colsample_bytree\n",
    "* colsample_bytree, default=1.0, type=double, 0.0 < colsample_bytree < 1.0, alias=sub_feature\n",
    "* LightGBM will random select part of features on each iteration if feature_fraction smaller than 1.0. For example, if * set to 0.8, will select 80% features before training each tree.\n",
    "* Can use this to speed up training\n",
    "* Can use this to deal with over-fit\n",
    "    * Smaller: overfit\n",
    "    * larger: underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_round:  262\n",
      "colsample_bytree:  0.3\n",
      "MAE       :  1150.08\n"
     ]
    }
   ],
   "source": [
    "rgr = lgb.LGBMRegressor(learning_rate=0.1,                             \n",
    "                        n_estimators=100000,\n",
    "                        num_leaves=num_leaves,\n",
    "                        min_child_samples = min_child_samples,\n",
    "                        colsample_bytree = 0.3)\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric=lgbm_eval_mae,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose = False)\n",
    "\n",
    "print 'best_round: ', rgr.best_iteration\n",
    "print 'colsample_bytree: ', rgr.colsample_bytree\n",
    "print 'MAE       : ', rgr.evals_result_.get('valid_0').get('mae')[rgr.best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colsample_bytree = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Tune subsample\n",
    "* default=0, type=int\n",
    "* Frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration.\n",
    "* Note: To enable bagging, should set subsample_freq as well (1 is recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_round:  179\n",
      "subsample:  0.9\n",
      "MAE       :  1150.57\n"
     ]
    }
   ],
   "source": [
    "rgr = lgb.LGBMRegressor(learning_rate=0.1,                             \n",
    "                        n_estimators=100000,\n",
    "                        num_leaves=num_leaves,\n",
    "                        min_child_samples = min_child_samples,\n",
    "                        colsample_bytree = colsample_bytree,\n",
    "                        subsample = 0.9,\n",
    "                        subsample_freq=1)\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric=lgbm_eval_mae,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose = False)\n",
    "\n",
    "print 'best_round: ', rgr.best_iteration\n",
    "print 'subsample: ', rgr.subsample\n",
    "print 'MAE       : ', rgr.evals_result_.get('valid_0').get('mae')[rgr.best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsample = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Tune max_bin\n",
    "* default=255, type=int\n",
    "* max number of bin that feature values will bucket in. Small bin may reduce training accuracy but may increase general power (deal with over-fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_round:  164\n",
      "max_bin:  8191\n",
      "MAE       :  1150.81\n"
     ]
    }
   ],
   "source": [
    "rgr = lgb.LGBMRegressor(learning_rate=0.1,                             \n",
    "                        n_estimators=100000,\n",
    "                        num_leaves=num_leaves,\n",
    "                        min_child_samples = min_child_samples,\n",
    "                        colsample_bytree = colsample_bytree,\n",
    "                        subsample = subsample,\n",
    "                        subsample_freq=1,\n",
    "                        max_bin = 8191 )\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric=lgbm_eval_mae,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose = False)\n",
    "\n",
    "y_pred = rgr.predict(X_val,num_iteration=rgr.best_iteration)\n",
    "print 'best_round: ', rgr.best_iteration\n",
    "print 'max_bin: ', rgr.max_bin\n",
    "print 'MAE       : ', rgr.evals_result_.get('valid_0').get('mae')[rgr.best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_bin = 8191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Tune reg_alpha\n",
    "* default=0, type=float\n",
    "* L1 regularization term on weights (deal with over-fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_round:  200\n",
      "reg_alpha:  0.01\n",
      "MAE       :  1150.29\n"
     ]
    }
   ],
   "source": [
    "rgr = lgb.LGBMRegressor(learning_rate=0.1,                             \n",
    "                        n_estimators=100000,\n",
    "                        num_leaves=num_leaves,\n",
    "                        min_child_samples = min_child_samples,\n",
    "                        colsample_bytree = colsample_bytree,\n",
    "                        subsample = subsample,\n",
    "                        subsample_freq=1,\n",
    "                        max_bin = max_bin,\n",
    "                        reg_alpha = 0.01)\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric=lgbm_eval_mae,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose = False)\n",
    "\n",
    "y_pred = rgr.predict(X_val,num_iteration=rgr.best_iteration)\n",
    "print 'best_round: ', rgr.best_iteration\n",
    "print 'reg_alpha: ', rgr.reg_alpha\n",
    "print 'MAE       : ', rgr.evals_result_.get('valid_0').get('mae')[rgr.best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_alpha = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated tuning - [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization)\n",
    "\n",
    "The idea is to set a range for each parameters, for which we can leverage the parameters from manual tuning, then let the bayersian optimization to seek best parameters.\n",
    "\n",
    "It's more efficient than grid search but is still time consuming. Therefore knowing an approximate range of values for each parameter will greatly improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgbm_cv(max_bin, num_leaves, min_child_samples, colsample_bytree, subsample, reg_alpha, learning_rate=0.1):\n",
    "    skf = list(KFold(len(train_y), 4))\n",
    "    scores=[]\n",
    "    for i, (train, val) in enumerate(skf):\n",
    "        est=lgb.LGBMRegressor(learning_rate=0.1,\n",
    "                        max_bin=int(max_bin),\n",
    "                        num_leaves=int(num_leaves),\n",
    "                        min_child_samples=int(min_child_samples),\n",
    "                        colsample_bytree=colsample_bytree,\n",
    "                        subsample=subsample,\n",
    "                        subsample_freq = 1,\n",
    "                        reg_alpha = reg_alpha\n",
    "                        )\n",
    " \n",
    "        train_x_fold = train_x.iloc[train]\n",
    "        train_y_fold = train_y[train]\n",
    "        val_x_fold = train_x.iloc[val]\n",
    "        val_y_fold = train_y[val]\n",
    "        est.set_params( n_estimators=100000)\n",
    "        est.fit(train_x_fold,\n",
    "                train_y_fold,\n",
    "                eval_set=[(val_x_fold, val_y_fold)],\n",
    "                eval_metric=lgbm_eval_mae,\n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False\n",
    "               )\n",
    "        val_y_predict_fold = est.predict(val_x_fold)\n",
    "        score = log_mae(val_y_fold, val_y_predict_fold,200)\n",
    "        scores.append(score)\n",
    "    return -np.mean(scores)\n",
    "\n",
    "\n",
    "lgbm_BO = BayesianOptimization(lgbm_cv, \n",
    "                               {\n",
    "                                'max_bin': (8167,10939),\n",
    "                                'num_leaves': (31,155),\n",
    "                                'min_child_samples' :(170,250),\n",
    "                                'colsample_bytree': (0.4,0.8),\n",
    "                                'subsample' : (0.9,1),\n",
    "                                'reg_alpha': (0,0.01)})\n",
    "\n",
    "lgbm_BO.maximize(init_points=5, n_iter=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model OHE Coding\n",
    "####  Categorical features\n",
    "1. Label Encoding (Factorizing)\n",
    "2. One Hot Encoding (get dummies)\n",
    "\n",
    "OHE can be done by either get_dummies() from Pandas package or OneHotEncoder from SK-Learn package. \n",
    "\n",
    "* get_dummies is easier to implement (can be used directly on raw categorical features, i.e. strings, but it takes longer time and is not memory efficient.\n",
    "\n",
    "* OneHotEncoder requires the features being converted to numeric, which has already been done by LabelEncoder in previous step, and is much more efficient (7x faster).\n",
    "\n",
    "* The OHE's results are converted to a sparse matrix which uses way less memory as compared to dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This step has been finished previously. Run this cell if start with fresh data.\n",
    "\n",
    "\n",
    "# start = time.time() \n",
    "# train_data = pd.read_csv('../input/train.csv')\n",
    "# train_size=train_data.shape[0]\n",
    "# print (\"Loading train data finished in %0.3fs\" % (time.time() - start))\n",
    "\n",
    "# start = time.time()\n",
    "# test_data = pd.read_csv('../input/test.csv')\n",
    "# print (\"Loading test data finished in %0.3fs\" % (time.time() - start))   \n",
    "\n",
    "# full_data=pd.concat([train_data,test_data])\n",
    "# del( train_data, test_data)\n",
    "# print (\"Full Data set created.\")\n",
    "\n",
    "# data_types = full_data.dtypes  \n",
    "# cat_cols = list(data_types[data_types=='object'].index)\n",
    "# num_cols = list(data_types[data_types=='int64'].index) + list(data_types[data_types=='float64'].index)\n",
    "\n",
    "# id_col = 'id'\n",
    "# target_col = 'loss'\n",
    "# num_cols.remove('id')\n",
    "# num_cols.remove('loss')\n",
    "\n",
    "# LBL = preprocessing.LabelEncoder()\n",
    "# start=time.time()\n",
    "# for cat_col in cat_cols:\n",
    "# #     print (\"Factorize feature %s\" % (cat))\n",
    "#     full_data[cat_col] = LBL.fit_transform(full_data[cat_col])\n",
    "# print ('Label enconding finished in %f seconds' % (time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot-encoding finished in 13.893000 seconds\n",
      "(313864, 1176)\n"
     ]
    }
   ],
   "source": [
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "start=time.time()\n",
    "full_data_sparse=OHE.fit_transform(full_data[cat_cols])\n",
    "print 'One-hot-encoding finished in %f seconds' % (time.time()-start)\n",
    "print full_data_sparse.shape\n",
    "\n",
    "## it should be (313864, 1176)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313864, 1190)\n"
     ]
    }
   ],
   "source": [
    "lift = 200\n",
    "\n",
    "full_data_sparse = sparse.hstack((full_data_sparse\n",
    "                                  ,full_data[num_cols])\n",
    "                                 , format='csr'\n",
    "                                 )\n",
    "print full_data_sparse.shape\n",
    "train_x = full_data_sparse[:train_size]\n",
    "test_x = full_data_sparse[train_size:]\n",
    "train_y = np.log(full_data[:train_size].loss.values + lift)\n",
    "ID = full_data.id[:train_size].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tune num_leaves\n",
    "* default=31, type=int, alias=num_leaf\n",
    "* number of leaves in one tree\n",
    "* control overfit\n",
    "    * Smaller: underfit\n",
    "    * larger: overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rgr = lgb.LGBMRegressor(learning_rate=0.1,                             \n",
    "                        n_estimators=100000,\n",
    "                        num_leaves=63)\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric=lgbm_eval_mae,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose = False)\n",
    "\n",
    "print 'best_round: ', rgr.best_iteration\n",
    "print 'num_leaves: ', rgr.num_leaves\n",
    "print 'MAE       : ', rgr.evals_result_.get('valid_0').get('mae')[rgr.best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_leaves = 63\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tune min_child_samples\n",
    "* default=10, type=int, alias=min_data_per_leaf , min_data\n",
    "* Minimal number of data in one leaf. Can use this to deal with over-fit.\n",
    "* control overfit\n",
    "    * Smaller: overfit\n",
    "    * larger: underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rgr = lgb.LGBMRegressor(learning_rate=0.1,                             \n",
    "                        n_estimators=100000,\n",
    "                        num_leaves=num_leaves,\n",
    "                        min_child_samples = 100)\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric=lgbm_eval_mae,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose = False)\n",
    "\n",
    "print 'best_round: ', rgr.best_iteration\n",
    "print 'min_child_samples: ', rgr.min_child_samples\n",
    "print 'MAE       : ', rgr.evals_result_.get('valid_0').get('mae')[rgr.best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_data_in_leaf = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tune colsample_bytree\n",
    "* colsample_bytree, default=1.0, type=double, 0.0 < colsample_bytree < 1.0, alias=sub_feature\n",
    "* LightGBM will random select part of features on each iteration if feature_fraction smaller than 1.0. For example, if * set to 0.8, will select 80% features before training each tree.\n",
    "* Can use this to speed up training\n",
    "* Can use this to deal with over-fit\n",
    "    * Smaller: overfit\n",
    "    * larger: underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rgr = lgb.LGBMRegressor(learning_rate=0.1,                             \n",
    "                        n_estimators=100000,\n",
    "                        num_leaves=num_leaves,\n",
    "                        min_child_samples = min_child_samples,\n",
    "                        colsample_bytree = 0.5)\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric=lgbm_eval_mae,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose = False)\n",
    "\n",
    "print 'best_round: ', rgr.best_iteration\n",
    "print 'colsample_bytree: ', rgr.colsample_bytree\n",
    "print 'MAE       : ', rgr.evals_result_.get('valid_0').get('mae')[rgr.best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colsample_bytree = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Tune subsample\n",
    "* default=0, type=int\n",
    "* Frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration.\n",
    "* Note: To enable bagging, should set subsample_freq as well (1 is recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rgr = lgb.LGBMRegressor(learning_rate=0.1,                             \n",
    "                        n_estimators=100000,\n",
    "                        num_leaves=num_leaves,\n",
    "                        min_child_samples = min_child_samples,\n",
    "                        colsample_bytree = colsample_bytree,\n",
    "                        subsample = 1,\n",
    "                        subsample_freq=1)\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric=lgbm_eval_mae,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose = False)\n",
    "\n",
    "print 'best_round: ', rgr.best_iteration\n",
    "print 'subsample: ', rgr.subsample\n",
    "print 'MAE       : ', rgr.evals_result_.get('valid_0').get('mae')[rgr.best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsample = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Tune max_bin\n",
    "* default=255, type=int\n",
    "* max number of bin that feature values will bucket in. Small bin may reduce training accuracy but may increase general power (deal with over-fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rgr = lgb.LGBMRegressor(learning_rate=0.1,                             \n",
    "                        n_estimators=100000,\n",
    "                        num_leaves=num_leaves,\n",
    "                        min_child_samples = min_child_samples,\n",
    "                        colsample_bytree = colsample_bytree,\n",
    "                        subsample = subsample,\n",
    "                        subsample_freq=1,\n",
    "                        max_bin = 511 )\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric=lgbm_eval_mae,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose = False)\n",
    "\n",
    "y_pred = rgr.predict(X_val,num_iteration=rgr.best_iteration)\n",
    "print 'best_round: ', rgr.best_iteration\n",
    "print 'max_bin: ', rgr.max_bin\n",
    "print 'MAE       : ', rgr.evals_result_.get('valid_0').get('mae')[rgr.best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_bin = 511"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Tune reg_alpha\n",
    "* default=0, type=float\n",
    "* L1 regularization term on weights (deal with over-fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rgr = lgb.LGBMRegressor(learning_rate=0.1,                             \n",
    "                        n_estimators=100000,\n",
    "                        num_leaves=num_leaves,\n",
    "                        min_child_samples = min_child_samples,\n",
    "                        colsample_bytree = colsample_bytree,\n",
    "                        subsample = subsample,\n",
    "                        subsample_freq=1,\n",
    "                        max_bin = max_bin,\n",
    "                        reg_alpha = 0.01)\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric=lgbm_eval_mae,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose = False)\n",
    "\n",
    "y_pred = rgr.predict(X_val,num_iteration=rgr.best_iteration)\n",
    "print 'best_round: ', rgr.best_iteration\n",
    "print 'reg_alpha: ', rgr.reg_alpha\n",
    "print 'MAE       : ', rgr.evals_result_.get('valid_0').get('mae')[rgr.best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_alpha = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated tuning - [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization)\n",
    "\n",
    "The idea is to set a range for each parameters, for which we can leverage the parameters from manual tuning, then let the bayersian optimization to seek best parameters.\n",
    "\n",
    "It's more efficient than grid search but is still time consuming. Therefore knowing an approximate range of values for each parameter will greatly improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgbm_cv(max_bin, num_leaves, min_child_samples, colsample_bytree, subsample, reg_alpha, learning_rate=0.1):\n",
    "    skf = list(KFold(len(train_y), 4))\n",
    "    scores=[]\n",
    "    for i, (train, val) in enumerate(skf):\n",
    "        est=lgb.LGBMRegressor(learning_rate=0.1,\n",
    "                        max_bin=int(max_bin),\n",
    "                        num_leaves=int(num_leaves),\n",
    "                        min_child_samples=int(min_child_samples),\n",
    "                        colsample_bytree=colsample_bytree,\n",
    "                        subsample=subsample,\n",
    "                        subsample_freq = 1,\n",
    "                        reg_alpha = reg_alpha\n",
    "                        )\n",
    " \n",
    "        train_x_fold = train_x.iloc[train]\n",
    "        train_y_fold = train_y[train]\n",
    "        val_x_fold = train_x.iloc[val]\n",
    "        val_y_fold = train_y[val]\n",
    "        est.set_params( n_estimators=100000)\n",
    "        est.fit(train_x_fold,\n",
    "                train_y_fold,\n",
    "                eval_set=[(val_x_fold, val_y_fold)],\n",
    "                eval_metric=lgbm_eval_mae,\n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False\n",
    "               )\n",
    "        val_y_predict_fold = est.predict(val_x_fold)\n",
    "        score = log_mae(val_y_fold, val_y_predict_fold,200)\n",
    "        scores.append(score)\n",
    "    return -np.mean(scores)\n",
    "            \n",
    "\n",
    "\n",
    "lgbm_BO = BayesianOptimization(lgbm_cv, {\n",
    "                                     'max_bin': (447,627),\n",
    "                                     'num_leaves': (60,180),\n",
    "                                     'min_child_samples' :(60,140),\n",
    "                                     'colsample_bytree': (0.15,0.6),\n",
    "                                     'subsample' : (0.6,1),\n",
    "                                     'reg_alpha': (0,0.01)})\n",
    "\n",
    "lgbm_BO.maximize(init_points=5, n_iter=30)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
