{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xg_eval_mae(yhat, dtrain, lift=0):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-lift, np.exp(yhat)-lift)\n",
    "\n",
    "def xgb_logregobj(preds, dtrain):\n",
    "    con = 2\n",
    "    labels = dtrain.get_label()\n",
    "    x =preds-labels\n",
    "    grad =con*x / (np.abs(x)+con)\n",
    "    hess =con**2 / (np.abs(x)+con)**2\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data finished in 3.027s\n",
      "Loading test data finished in 1.927s\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "train_data = pd.read_csv('../input/train.csv')\n",
    "train_size=train_data.shape[0]\n",
    "print (\"Loading train data finished in %0.3fs\" % (time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "print (\"Loading test data finished in %0.3fs\" % (time.time() - start))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge train and test\n",
    "Save our time on duplicating logics for train and test and will also ensure the transformations applied on train and test are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data set created.\n"
     ]
    }
   ],
   "source": [
    "full_data=pd.concat([train_data,test_data])\n",
    "del( train_data, test_data)\n",
    "print (\"Full Data set created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_types = full_data.dtypes  \n",
    "cat_cols = list(data_types[data_types=='object'].index)\n",
    "num_cols = list(data_types[data_types=='int64'].index) + list(data_types[data_types=='float64'].index)\n",
    "\n",
    "id_col = 'id'\n",
    "target_col = 'loss'\n",
    "num_cols.remove('id')\n",
    "num_cols.remove('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric features\n",
    "\n",
    "Two preprocessings on numeric features are applied:\n",
    "\n",
    "1. Apply box-cox transformations for skewed numeric features.\n",
    "\n",
    "2. Scale numeric features so they will fall in the range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skewed_cols = full_data[num_cols].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "SSL = preprocessing.StandardScaler()\n",
    "skewed_cols = skewed_cols[skewed_cols > 0.25].index.values\n",
    "for skewed_col in skewed_cols:\n",
    "    full_data[skewed_col], lam = boxcox(full_data[skewed_col] + 1)\n",
    "for num_col in num_cols:\n",
    "    full_data[num_col] = SSL.fit_transform(full_data[num_col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model LE Coding\n",
    "### Categorical features\n",
    "1. Label Encoding (Factorizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label enconding finished in 37.017625 seconds\n"
     ]
    }
   ],
   "source": [
    "LBL = preprocessing.LabelEncoder()\n",
    "start=time.time()\n",
    "for cat_col in cat_cols:\n",
    "    full_data[cat_col] = LBL.fit_transform(full_data[cat_col])\n",
    "print ('Label enconding finished in %f seconds' % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lift = 200\n",
    "\n",
    "train_x = full_data[cat_cols + num_cols][:train_size]\n",
    "test_x = full_data[cat_cols + num_cols][train_size:]\n",
    "train_y = np.log(full_data[:train_size].loss.values + lift)\n",
    "ID = full_data.id[:train_size].values\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y) #used for Bayersian Optimization\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [XGBoost](http://xgboost.readthedocs.io/en/latest/) tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual tuning \n",
    "1. [Bayersian Optimization](https://github.com/fmfn/BayesianOptimization) will be introduced to optimize each parameter. \n",
    "2. Based on holdout data (X_val, y_val) for validation for the sake of time. The range for each parameter can be estimated based on the manual tuning results. However, grid search can also be used if time allows.\n",
    "3. A larger learning rate (0.1) is implemented since it requires less iterations to complete. A smaller one (0.01 or even smaller) will be used to train the model for better accuracy when the parameters are optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tune max_depth\n",
    "**max_depth : int**\n",
    "\n",
    "    Maximum tree depth for base learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth:\t5\n",
      "\n",
      "score:\t\t1154.561768\n",
      "\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learning_rate = 0.1\n",
    "rgr = xgb.XGBRegressor(\n",
    "    seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "    learning_rate = learning_rate,\n",
    "    n_estimators = 10000,\n",
    "    max_depth= 5,\n",
    "    nthread = -1,\n",
    "    silent = False\n",
    ")\n",
    "rgr.fit(\n",
    "    X_train,y_train,\n",
    "    eval_set=[(X_val,y_val)],\n",
    "    eval_metric=xg_eval_mae,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print 'max_depth:\\t{}\\n'.format(rgr.get_xgb_params()['max_depth'])\n",
    "print 'score    :\\t{}\\n'.format(rgr.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results are listed as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    max_depth  score\n",
    "    5          1154.56\n",
    "    6          1156.13\n",
    "    7          1155.86\n",
    "    8          1156.74\n",
    "    9          1158.39\n",
    "    10         1161.08\n",
    "    11         1164.82\n",
    "    12         1168.76\n",
    "    4          1157.56 \n",
    "    3          1163.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depth = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tune min_child_weight\n",
    "**min_child_weight : int**\n",
    "    \n",
    "    Minimum sum of instance weight(hessian) needed in a child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_child_weight:\t105\n",
      "\n",
      "score           :\t1150.430908\n",
      "\n",
      "Wall time: 4min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rgr = xgb.XGBRegressor(\n",
    "    seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "    learning_rate = learning_rate,\n",
    "    n_estimators = 10000,\n",
    "    max_depth=max_depth,\n",
    "    min_child_weight=105,\n",
    "    nthread = -1,\n",
    "    silent = False\n",
    ")\n",
    "rgr.fit(\n",
    "    X_train,y_train,\n",
    "    eval_set=[(X_val,y_val)],\n",
    "    eval_metric=xg_eval_mae,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "print 'min_child_weight:\\t{}\\n'.format(rgr.get_xgb_params()['min_child_weight'])\n",
    "print 'score           :\\t{}\\n'.format(rgr.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results are listed as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    min_child_weight  score\n",
    "    1                 1153.27\n",
    "    5                 1155.26\n",
    "    10                1156.5\n",
    "    15                1155.37\n",
    "    20                1153.96\n",
    "    25                1155.06\n",
    "    30                1152.18\n",
    "    35                1153.08\n",
    "    40                1154.31\n",
    "    45                1153.62\n",
    "    50                1153.89\n",
    "    55                1153.72\n",
    "    60                1152.41\n",
    "    65                1152.57\n",
    "    70                1152.41\n",
    "    75                1152.51\n",
    "    80                1152.07\n",
    "    85                1152.13\n",
    "    90                1152.45\n",
    "    95                1151.88\n",
    "    100               1153.39\n",
    "    105               1150.43\n",
    "    110               1152.67\n",
    "    115               1151.76\n",
    "    120               1150.9\n",
    "    125               1152.07\n",
    "    130               1151.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_child_weight = 105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.Tune colsample_bytree\n",
    "\n",
    "**colsample_bytree : float**\n",
    "\n",
    "    Subsample ratio of columns when constructing each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colsample_bytree:\t0.8\n",
      "\n",
      "score           :\t1153.147461\n",
      "\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rgr = xgb.XGBRegressor(\n",
    "    seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "    learning_rate = learning_rate,\n",
    "    n_estimators = 10000,\n",
    "    max_depth=max_depth,\n",
    "    min_child_weight=min_child_weight,\n",
    "    colsample_bytree=0.8,\n",
    "    nthread = -1,\n",
    "    silent = False\n",
    ")\n",
    "\n",
    "\n",
    "rgr.fit(\n",
    "    X_train,y_train,\n",
    "    eval_set=[(X_val,y_val)],\n",
    "    eval_metric=xg_eval_mae,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "print 'colsample_bytree:\\t{}\\n'.format(rgr.get_xgb_params()['colsample_bytree'])\n",
    "print 'score           :\\t{}\\n'.format(rgr.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The results are listed as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    colsample_bytree  score\n",
    "    1                 1150.43\n",
    "    0.9               1150.85\n",
    "    0.8               1153.15\n",
    "    0.7               1154.88\n",
    "    0.6               1153.1\n",
    "    0.5               1153.89\n",
    "    0.4               1152.61\n",
    "    0.3               1152.56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The value 0.8 was chosen to introduce randomization \n",
    "# since it is the objective to find the range of each parameter.\n",
    "colsample_bytree = 0.8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Tune subsample\n",
    "**subsample : float**\n",
    "\n",
    "    Subsample ratio of the training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample:\t0.8\n",
      "\n",
      "score    :\t1153.380859\n",
      "\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rgr = xgb.XGBRegressor(\n",
    "    seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "    learning_rate = learning_rate,\n",
    "    n_estimators = 10000,\n",
    "    max_depth=max_depth,\n",
    "    min_child_weight=min_child_weight,\n",
    "    colsample_bytree=colsample_bytree,\n",
    "    subsample=0.8,\n",
    "    nthread = -1,\n",
    "    silent = False\n",
    ")\n",
    "\n",
    "\n",
    "rgr.fit(\n",
    "    X_train,y_train,\n",
    "    eval_set=[(X_val,y_val)],\n",
    "    eval_metric=xg_eval_mae,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print 'subsample:\\t{}\\n'.format(rgr.get_xgb_params()['subsample'])\n",
    "print 'score    :\\t{}\\n'.format(rgr.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    subsample score\n",
    "    1         1153.15\n",
    "    0.9       1150.88\n",
    "    0.8       1153.38\n",
    "    0.7       1154.21\n",
    "    0.6       1156.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The value 0.8 was chosen to introduce randomization \n",
    "# since it is the objective to find the range of each parameter.\n",
    "subsample = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Tune gamma\n",
    "**gamma : float**\n",
    "\n",
    "    Minimum loss reduction required to make a further partition on a leaf node of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma:\t1.2\n",
      "\n",
      "score:\t1151.082642\n",
      "\n",
      "Wall time: 4min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rgr = xgb.XGBRegressor(\n",
    "    seed = 1234,\n",
    "    learning_rate = learning_rate,\n",
    "    n_estimators = 10000,\n",
    "    max_depth=max_depth,\n",
    "    min_child_weight=min_child_weight,\n",
    "    colsample_bytree=colsample_bytree,\n",
    "    subsample=subsample,\n",
    "    gamma=1.2,\n",
    "    nthread = -1,\n",
    "    silent = False\n",
    ")\n",
    "\n",
    "rgr.fit(\n",
    "    X_train,y_train,\n",
    "    eval_set=[(X_val,y_val)],\n",
    "    eval_metric=xg_eval_mae,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "print 'gamma:\\t{}\\n'.format(rgr.get_xgb_params()['gamma'])\n",
    "print 'score:\\t{}\\n'.format(rgr.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    gamma score\n",
    "    0     1153.38\n",
    "    0.3   1153.13\n",
    "    0.6   1151.36\n",
    "    0.9   1152.17\n",
    "    1.2   1151.08\n",
    "    1.5   1151.47\n",
    "    1.8   1154.93\n",
    "    2.1   1152.41\n",
    "    2.4   1154.99\n",
    "    2.7   1156.14\n",
    "    3     1156.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated tuning - [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization)\n",
    "\n",
    "The idea is to set a range for each parameters, for which we can leverage the parameters from manual tuning, then let the bayersian optimization to seek best parameters.\n",
    "\n",
    "It's more efficient than grid search but is still time consuming, which took me over 20 hours to finish the optimization. Therefore knowing an approximate range of values for each parameter will greatly improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, subsample, gamma):\n",
    "    params = dict()\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=10000, nfold=4,\n",
    "        feval = xg_eval_mae,obj = xgb_logregobj,\n",
    "        seed=1234,callbacks=[xgb.callback.early_stop(50)]\n",
    "    )\n",
    "    \n",
    "    return -cv_result['test-mae-mean'].values[-1]\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {\n",
    "        'max_depth': (max_depth - 1, max_depth + 3),\n",
    "        'min_child_weight': (min_child_weight - 20, min_child_weight + 20),\n",
    "        'colsample_bytree': (max(colsample_bytree - 0.2, 0.1), min(colsample_bytree + 0.2, 1)),\n",
    "        'subsample': (max(subsample - 0.2, 0.1), min(subsample + 0.2, 1)),\n",
    "        'gamma': (max(gamma - 0.25, 0), gamma + 0.4)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.maximize(init_points=5, n_iter=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best sets of parameters are listed:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(max_depth=7,min_child_weight=88,colsample_bytree=0.615498,subsample=0.810715,gamma=1.562494)\n",
    "#               score -1143.321167\n",
    "(max_depth=6,min_child_weight=115,colsample_bytree=0.855791,subsample=0.916137,gamma=1.357693)\n",
    "#               score -1144.113800\n",
    "(max_depth=8,min_child_weight=102,colsample_bytree=0.600000,subsample=1.000000,gamma=0.950000)\n",
    "#               score -1144.603485\n",
    "(max_depth=8,min_child_weight=113,colsample_bytree=0.600000,subsample=1.000000,gamma=0.950000)\n",
    "#               score -1145.356110  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model OHE Coding\n",
    "####  Categorical features\n",
    "1. Label Encoding (Factorizing)\n",
    "2. One Hot Encoding (get dummies)\n",
    "\n",
    "OHE can be done by either get_dummies() from Pandas package or OneHotEncoder from SK-Learn package. \n",
    "\n",
    "* get_dummies is easier to implement (can be used directly on raw categorical features, i.e. strings, but it takes longer time and is not memory efficient.\n",
    "\n",
    "* OneHotEncoder requires the features being converted to numeric, which has already been done by LabelEncoder in previous step, and is much more efficient (7x faster).\n",
    "\n",
    "* The OHE's results are converted to a sparse matrix which uses way less memory as compared to dense matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This step has been finished previously. Run this cell if start with fresh data.\n",
    "\n",
    "\n",
    "# start = time.time() \n",
    "# train_data = pd.read_csv('../input/train.csv')\n",
    "# train_size=train_data.shape[0]\n",
    "# print (\"Loading train data finished in %0.3fs\" % (time.time() - start))\n",
    "\n",
    "# start = time.time()\n",
    "# test_data = pd.read_csv('../input/test.csv')\n",
    "# print (\"Loading test data finished in %0.3fs\" % (time.time() - start))   \n",
    "\n",
    "# full_data=pd.concat([train_data,test_data])\n",
    "# del( train_data, test_data)\n",
    "# print (\"Full Data set created.\")\n",
    "\n",
    "# data_types = full_data.dtypes  \n",
    "# cat_cols = list(data_types[data_types=='object'].index)\n",
    "# num_cols = list(data_types[data_types=='int64'].index) + list(data_types[data_types=='float64'].index)\n",
    "\n",
    "# id_col = 'id'\n",
    "# target_col = 'loss'\n",
    "# num_cols.remove('id')\n",
    "# num_cols.remove('loss')\n",
    "\n",
    "# LBL = preprocessing.LabelEncoder()\n",
    "# start=time.time()\n",
    "# for cat_col in cat_cols:\n",
    "# #     print (\"Factorize feature %s\" % (cat))\n",
    "#     full_data[cat_col] = LBL.fit_transform(full_data[cat_col])\n",
    "# print ('Label enconding finished in %f seconds' % (time.time()-start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot-encoding finished in 10.411293 seconds\n",
      "(313864, 1176)\n"
     ]
    }
   ],
   "source": [
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "start=time.time()\n",
    "full_data_sparse=OHE.fit_transform(full_data[cat_cols])\n",
    "print ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n",
    "print (full_data_sparse.shape)\n",
    "\n",
    "## it should be (313864, 1176)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313864, 1190)\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "lift = 200\n",
    "\n",
    "full_data_sparse = sparse.hstack((full_data_sparse\n",
    "                                  ,full_data[num_cols])\n",
    "                                 , format='csr'\n",
    "                                 )\n",
    "print (full_data_sparse.shape)\n",
    "train_x = full_data_sparse[:train_size]\n",
    "test_x = full_data_sparse[train_size:]\n",
    "train_y = np.log(full_data[:train_size].loss.values + lift)\n",
    "ID = full_data.id[:train_size].values\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y) #used for Bayersian Optimization\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## [XGBoost](http://xgboost.readthedocs.io/en/latest/) tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1. Tune max_depth\n",
    "**max_depth : int**\n",
    "\n",
    "    Maximum tree depth for base learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "rgr = xgb.XGBRegressor(\n",
    "    seed = 1234, \n",
    "    objective = logregobj,\n",
    "    learning_rate = learning_rate,\n",
    "    n_estimators = 10000,\n",
    "    max_depth=13,\n",
    "    nthread = -1,\n",
    "    silent = False\n",
    ")\n",
    "rgr.fit(\n",
    "    X_train,y_train,\n",
    "    eval_set=[(X_val,y_val)],\n",
    "    eval_metric=xg_eval_mae,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print 'max_depth:\\t{}\\n'.format(rgr.get_xgb_params()['max_depth'])\n",
    "print 'score    :\\t{}\\n'.format(rgr.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    max_depth score\n",
    "    5         1163.51\n",
    "    6         1167.68\n",
    "    7         1169.18 \n",
    "    8         1169.68 \n",
    "    10        1175.96 \n",
    "    15        1189.57 \n",
    "    12        1177.27\n",
    "    11        1175.76\n",
    "    9         1172.38\n",
    "    13        1180.775757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depth = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tune min_child_weight\n",
    "**min_child_weight : int**\n",
    "    \n",
    "    Minimum sum of instance weight(hessian) needed in a child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rgr = xgb.XGBRegressor(\n",
    "    seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "    learning_rate = learning_rate,\n",
    "    n_estimators = 10000,\n",
    "    max_depth=max_depth,\n",
    "    min_child_weight=105,\n",
    "    nthread = -1,\n",
    "    silent = False\n",
    ")\n",
    "rgr.fit(\n",
    "    X_train,y_train,\n",
    "    eval_set=[(X_val,y_val)],\n",
    "    eval_metric=xg_eval_mae,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "print 'min_child_weight:\\t{}\\n'.format(rgr.get_xgb_params()['min_child_weight'])\n",
    "print 'score           :\\t{}\\n'.format(rgr.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    min_child_weight score \n",
    "    15               1157.88\n",
    "    5                1161.54\n",
    "    1                1163.46\n",
    "    10               1161.81\n",
    "    20               1159.91\n",
    "    25               1158.85\n",
    "    30               1158.3\n",
    "    40               1155.78\n",
    "    45               1158.47\n",
    "    50               1157.56\n",
    "    55               1157.01\n",
    "    60               1156.45\n",
    "    65               1158.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_child_weight = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Tune colsample_bytree\n",
    "\n",
    "**colsample_bytree : float**\n",
    "\n",
    "    Subsample ratio of columns when constructing each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rgr = xgb.XGBRegressor(\n",
    "    seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "    learning_rate = learning_rate,\n",
    "    n_estimators = 10000,\n",
    "    max_depth=max_depth,\n",
    "    min_child_weight=min_child_weight,\n",
    "    colsample_bytree=0.8,\n",
    "    nthread = -1,\n",
    "    silent = False\n",
    ")\n",
    "\n",
    "\n",
    "rgr.fit(\n",
    "    X_train,y_train,\n",
    "    eval_set=[(X_val,y_val)],\n",
    "    eval_metric=xg_eval_mae,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "print 'colsample_bytree:\\t{}\\n'.format(rgr.get_xgb_params()['colsample_bytree'])\n",
    "print 'score           :\\t{}\\n'.format(rgr.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    colsample_bytree score\n",
    "    1                1155.78\n",
    "    0.9              1164.98\n",
    "    0.8              1160.84\n",
    "    0.7              1164.93\n",
    "    0.6              1157.09\n",
    "    0.5              1159.3\n",
    "    0.4              1158.58\n",
    "    0.3              1160.91\n",
    "    0.2              1155.38\n",
    "    0.1              1159.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colsample_bytree = 0.2 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Tune subsample\n",
    "**subsample : float**\n",
    "\n",
    "    Subsample ratio of the training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rgr = xgb.XGBRegressor(\n",
    "    seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "    learning_rate = learning_rate,\n",
    "    n_estimators = 10000,\n",
    "    max_depth=max_depth,\n",
    "    min_child_weight=min_child_weight,\n",
    "    colsample_bytree=colsample_bytree,\n",
    "    subsample=0.8,\n",
    "    nthread = -1,\n",
    "    silent = False\n",
    ")\n",
    "\n",
    "\n",
    "rgr.fit(\n",
    "    X_train,y_train,\n",
    "    eval_set=[(X_val,y_val)],\n",
    "    eval_metric=xg_eval_mae,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print 'subsample:\\t{}\\n'.format(rgr.get_xgb_params()['subsample'])\n",
    "print 'score    :\\t{}\\n'.format(rgr.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    subsample score\n",
    "    1         1155.38\n",
    "    0.9       1157.38\n",
    "    0.8       1154.79\n",
    "    0.7       1157.225586\n",
    "    0.6       1158.565796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsample =  0.8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Tune gamma\n",
    "**gamma : float**\n",
    "\n",
    "    Minimum loss reduction required to make a further partition on a leaf node of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rgr = xgb.XGBRegressor(\n",
    "    seed = 1234,\n",
    "    learning_rate = learning_rate,\n",
    "    n_estimators = 10000,\n",
    "    max_depth=max_depth,\n",
    "    min_child_weight=min_child_weight,\n",
    "    colsample_bytree=colsample_bytree,\n",
    "    subsample=subsample,\n",
    "    gamma=1.2,\n",
    "    nthread = -1,\n",
    "    silent = False\n",
    ")\n",
    "\n",
    "rgr.fit(\n",
    "    X_train,y_train,\n",
    "    eval_set=[(X_val,y_val)],\n",
    "    eval_metric=xg_eval_mae,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "print 'gamma:\\t{}\\n'.format(rgr.get_xgb_params()['gamma'])\n",
    "print 'score:\\t{}\\n'.format(rgr.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    gamma score\n",
    "    0     1154.79\n",
    "    0.3   1154.103394\n",
    "    0.6   1153.088135\n",
    "    0.9   1155.900391\n",
    "    1.2   1156.186035\n",
    "    1.5   1155.787231\n",
    "    1.8   1158.119263\n",
    "    2.1   1160.725952\n",
    "    2.4   1164.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated tuning - [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization)\n",
    "\n",
    "The idea is to set a range for each parameters, for which we can leverage the parameters from manual tuning, then let the bayersian optimization to seek best parameters.\n",
    "\n",
    "It's more efficient than grid search but is still time consuming, which took me over 20 hours to finish the optimization. Therefore knowing an approximate range of values for each parameter will greatly improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_evaluate(min_child_weight,colsample_bytree,max_depth,subsample,gamma):\n",
    "    params = {\n",
    "    'eta': 0.1,\n",
    "    'silent': 1,\n",
    "    'verbose_eval': True,\n",
    "    'seed': 1234\n",
    "    }\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, num_boost_round=10000, nfold=4,\n",
    "        obj = xgb_logregobj, feval=xg_eval_mae,\n",
    "        seed=1234,callbacks=[xgb.callback.early_stop(50)])\n",
    "    return -cv_result['test-mae-mean'].values[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {'max_depth': (11 , 18),\n",
    "     'min_child_weight': (100, 200),\n",
    "     'colsample_bytree': (0.05, 0.3),\n",
    "     'subsample': (0.6, 1),\n",
    "     'gamma': (0.5, 2.0)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.initialize({\n",
    "        -1147.727325: {'max_depth': 5.000000, 'min_child_weight': 60.000000, 'subsample':0.800000,\n",
    "                      'colsample_bytree':0.800000, 'gamma':0.000000},\n",
    "        -1143.200348: {'max_depth': 11.312448, 'min_child_weight': 119.388715, 'subsample':0.916137,\n",
    "                      'colsample_bytree':0.209870, 'gamma':0.564498},\n",
    "        -1141.388184: {'max_depth': 14.000000, 'min_child_weight': 140.000000, 'subsample':1.000000,\n",
    "                      'colsample_bytree':0.050000, 'gamma':0.900000},\n",
    "        \n",
    "    })\n",
    "\n",
    "xgb_BO.maximize(init_points=5, n_iter=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best sets of parameters are listed:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(max_depth=11,min_child_weight=200,colsample_bytree=0.05,subsample=1.0,gamma=0.5)\n",
    "#               score -1140.645843\n",
    "(max_depth=15,min_child_weight=177,colsample_bytree=0.209870,subsample=0.916137,gamma=0.916137)\n",
    "#               score -1140.646576\n",
    "(max_depth=13,min_child_weight=127,colsample_bytree=0.087842,subsample=0.969313,gamma=0.573770)\n",
    "#               score -1141.266846\n",
    "(max_depth=16,min_child_weight=130,colsample_bytree=0.060157,subsample=0.971690,gamma=0.676203)\n",
    "#               score -1142.136109"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
